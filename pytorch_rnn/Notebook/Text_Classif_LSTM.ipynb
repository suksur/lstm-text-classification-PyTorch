{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkJiNmXFDI6K"
   },
   "source": [
    "# Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk==3.7\n",
    "# !pip install numpy==1.21.5\n",
    "# !pip install pandas==1.3.5\n",
    "# !pip install scikit_learn==1.0.2\n",
    "# !pip install tensorflow==2.7.0\n",
    "# !pip install torch==1.10.2\n",
    "# !pip install matplotlib==3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqW-HqXiVVpU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "# import tensorflow as tf\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "from nltk import PorterStemmer\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AX3D0BeaDXjO"
   },
   "source": [
    "# Reading the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rDEMAW8Vq9e"
   },
   "outputs": [],
   "source": [
    "#reading data\n",
    "data = pd.read_csv('../Input/review_data (1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZaReM1qDiKY"
   },
   "source": [
    "# Information About Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rg1YLD7xVyEg"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Dmc1zuaWKV4"
   },
   "outputs": [],
   "source": [
    "#\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UotzM64FV4b8"
   },
   "outputs": [],
   "source": [
    " # Selecting the needed Column\n",
    "\n",
    "data = data[['content','score']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbFMlvE_VuK8"
   },
   "source": [
    "# Function to Clean the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tN71AhDCWBif"
   },
   "outputs": [],
   "source": [
    " # Defining a Function to Clean the Textual Data\n",
    " \n",
    "def clean_text(txt):\n",
    "    \n",
    "  txt = txt.lower() #Lowering the text\n",
    "  txt = re.sub(r'\\W', ' ', str(txt)) # remove all special characters including apastrophie \n",
    "  txt = txt.translate(str.maketrans('', '', string.punctuation)) # remove punctuations\n",
    "  txt = ''.join([i for i in txt if not i.isdigit()]).strip() # remove digits ()\n",
    "  txt = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', txt)   # remove all single characters (it's -> it s then we need to remove s)\n",
    "  txt = re.sub(r'\\s+', ' ', txt, flags=re.I) # Substituting multiple spaces with single space\n",
    "  txt = re.sub(r\"(http\\S+|http)\", \"\", txt) # remove links\n",
    "  txt = ' '.join([PorterStemmer().stem(word=word) for word in txt.split(\" \") if word not in stopwords.words('english') ]) # stem & remove stop words\n",
    "  return txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OZGtypvV1FK"
   },
   "source": [
    "# Comparison Between Orginal Text and Processed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SamWezXOYtTw"
   },
   "outputs": [],
   "source": [
    "print('Original Text : ',data['content'][1])  \n",
    "print('Processed Text : ',clean_text(data['content'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFFaQRJlWAle"
   },
   "source": [
    "# Applying the Function to the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUsM5_YTY21t"
   },
   "outputs": [],
   "source": [
    "data['content'] = data['content'].apply(clean_text) #apply the function to every text in the dataset\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO5yFh24WIdQ"
   },
   "source": [
    "# Distribution of Classes in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YWBMroMp6Rq"
   },
   "outputs": [],
   "source": [
    "data.score.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKNq2aBdW82z"
   },
   "outputs": [],
   "source": [
    "# we can see that class \"5\"  is dominating in the dataset. Thus we need to Balance the Dataset.\n",
    "\n",
    "pd.value_counts(data['score']).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8ZlGAsdY9Tj"
   },
   "source": [
    "# Balancing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5RrvGtC4WYF"
   },
   "outputs": [],
   "source": [
    "# First we need to divide the dataset to each classes\n",
    "\n",
    "df_majority = data[data['score']==5] #Data with class 5\n",
    "\n",
    "df_minority1= data[data['score']==2] #Data with class 2\n",
    "\n",
    "df_minority2 = data[data['score']==3] #Data with class 3\n",
    "\n",
    "df_minority3 = data[data['score']==1] #Data with class 1\n",
    "\n",
    "df_minority4 = data[data['score']==4] #Data with class 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPYCp1ocsfBa"
   },
   "source": [
    "# Upasampling the Monority class and Downsampling the Majority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDn58dHc5C0X"
   },
   "outputs": [],
   "source": [
    "#Down Sampling Majority Class \"5\"\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    \n",
    "                                 n_samples = 600)\n",
    "#Upsample Minority class  \"2\"\n",
    "df_minority_upsampled = resample(df_minority1, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=200)\n",
    "#Upsample Minority class \"3\"\n",
    "df_minority_upsampled1 = resample(df_minority2, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=300)\n",
    "#Upsample Minority class \"1\"\n",
    "df_minority_upsampled2 = resample(df_minority3, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=225)\n",
    "#Upsample Minority class \"4\"\n",
    "df_minority_upsampled3 = resample(df_minority4, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=250)\n",
    "\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "data1 = pd.concat([df_majority_downsampled, df_minority_upsampled,df_minority_upsampled1,df_minority_upsampled2,df_minority_upsampled3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8URYtTbo5gGy"
   },
   "outputs": [],
   "source": [
    "data1.score.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTvo6bptstDI"
   },
   "source": [
    "# Now we have a Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i56RRXCaRAj"
   },
   "outputs": [],
   "source": [
    "pd.value_counts(data1['score']).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtasSCdYiXlS"
   },
   "source": [
    "# Defining the Parameters and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.0.0 --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjp7SOYz8wzG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 2000\n",
    "# Max number of words in each Content.\n",
    "MAX_SEQUENCE_LENGTH = 600\n",
    "# This is fixed. Embedding\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(data1['content'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_9tXY3wZXOR"
   },
   "outputs": [],
   "source": [
    " #Tokenizing the content\n",
    " \n",
    "X = tokenizer.texts_to_sequences(data1['content'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAIiqpSTahgu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQogXdMRZuIv"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(data1['score'])\n",
    "print(Y.shape)\n",
    "print(le.classes_)\n",
    "Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz49zx5Qcff1"
   },
   "source": [
    "# Splitting Dataset to Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZjjAcnCZz4B"
   },
   "outputs": [],
   "source": [
    " #Train and Test Split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10, random_state = 42, stratify=Y)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBJH19RChkqb"
   },
   "outputs": [],
   "source": [
    "# Y_test = torch.Tensor(Y_test.to_numpy())\n",
    "# Y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tazgv11caEwf"
   },
   "outputs": [],
   "source": [
    " # Converting data into Torch and getting it into CPU\n",
    "\n",
    "x_train = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train = torch.tensor(Y_train, dtype=torch.long)\n",
    "x_cv = torch.tensor(X_test, dtype=torch.long)\n",
    "y_cv = torch.tensor(Y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xX-AIui-s_mr"
   },
   "source": [
    "# Converting dataset to a Torch Datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0CDy5I6KafQk"
   },
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(x_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJTZPvq9bSXW"
   },
   "outputs": [],
   "source": [
    "# Defing the Parameters:\n",
    "max_features =  2000  \n",
    "batch_size = 50\n",
    "vocab_size = max_features\n",
    "\n",
    "# Initialising the DataLoaders\n",
    "train_dl = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pEF_VtECs_w"
   },
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DJNQS-qbWs5"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LSTM(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) : # \n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        #self.drop  = nn.Dropout(p=0.2)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, 64, batch_first=True)\n",
    "        self.linear = nn.Linear(64, 5)\n",
    "        #self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        #x = self.drop(x)\n",
    "        out_pack, (ht, ct) = self.lstm(x)\n",
    "        out_pack1, (ht, ct) = self.lstm1(out_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        #out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bD2s9P7CfKz"
   },
   "source": [
    "# Structure of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0L-QyIXbbfL5"
   },
   "outputs": [],
   "source": [
    "#intializing model\n",
    "model = LSTM(vocab_size, 128,64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU8QYYE0CHOf"
   },
   "source": [
    "# Model Tranining And Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6b_K0nCbkQQ",
    "outputId": "e238b695-f5f8-4fbd-fb8f-dbc23bf33d04"
   },
   "outputs": [],
   "source": [
    "# Defining Train Loop:\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Loss Function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Optimiser\n",
    "\n",
    "# model.cuda() # Moving Model Into GPU\n",
    "# loss_fn.cuda() # Moving Loss Function Into GPU\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "for epoch in range(n_epochs):\n",
    "      start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    # Set model to train configuration\n",
    "      model.train() # indicator for training\n",
    "      avg_loss = 0.  \n",
    "      for i, (x_batch, y_batch) in enumerate(train_dl):\n",
    "#           x_batch = x_batch.cuda()\n",
    "#           y_batch = y_batch.cuda()\n",
    "\n",
    "\n",
    "          # Predict/Forward Pass\n",
    "          y_pred = model(x_batch)\n",
    "\n",
    "\n",
    "\n",
    "          # Compute loss\n",
    "          loss = loss_fn(y_pred, y_batch)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward() \n",
    "          optimizer.step()\n",
    "          avg_loss += loss.item() / len(train_dl)\n",
    "          \n",
    "\n",
    "\n",
    "\n",
    "      # Set model to validation configuration\n",
    "      model.eval() # Indicator for Validation       \n",
    "      avg_val_loss = 0.\n",
    "      val_preds = np.zeros((len(x_cv),len(le.classes_)))\n",
    "    \n",
    "      for i, (x_batch, y_batch) in enumerate(val_dl):\n",
    "          y_pred = model(x_batch).detach()\n",
    "          avg_val_loss += loss_fn(y_pred, y_batch).item() / len(val_dl)\n",
    "\n",
    "           #keep/store predictions\n",
    "\n",
    "          val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()\n",
    "          \n",
    "          # Check Accuracy\n",
    "      val_accuracy = sum(val_preds.argmax(axis=1)==Y_test)/len(Y_test)\n",
    "      train_loss.append(avg_loss)\n",
    "      valid_loss.append(avg_val_loss)\n",
    "      elapsed_time = time.time() - start_time \n",
    "      print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f}  \\t val_acc={:.4f}  \\t time={:.2f}s'.format(\n",
    "                epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmoyY-cJjQoo"
   },
   "source": [
    "#Plotting the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SMrXaqS_QG9"
   },
   "outputs": [],
   "source": [
    "#loss vs epoch graph\n",
    "def plot_graph(epochs):\n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "    plt.title(\"Train/Validation Loss\")\n",
    "    plt.plot(list(np.arange(epochs) + 1) , train_loss, label='train') # X axis \n",
    "    plt.plot(list(np.arange(epochs) + 1), valid_loss, label='validation') # Y axis\n",
    "    plt.xlabel('num_epochs', fontsize=12)\n",
    "    plt.ylabel('loss', fontsize=12)\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    },
    "id": "tyoMnxjjh6Cg",
    "outputId": "749cf4c3-c98c-4a8c-9664-00c5e219e9f2"
   },
   "outputs": [],
   "source": [
    "plot_graph(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_Classif_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
